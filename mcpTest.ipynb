{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f992ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Projects\\PersonalSite\\PSWindows\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.checkpoint.memory import MemorySaver  # MemorySaver() suporta async\n",
    "from langchain.messages import HumanMessage\n",
    "from langchain.agents import create_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eecd5af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57f99e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Config - habilite streaming\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-5-nano-2025-08-07\",  # gpt-5-nano pode não existir, use gpt-4o-mini\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    temperature=0.7,\n",
    "    streaming=True  # Necessário para token streaming\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fcbbba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCP Config\n",
    "mcp_config = {\n",
    "    \"github\": {\n",
    "        \"transport\": \"streamable_http\",  # transporte HTTP suportando headers\n",
    "        \"url\": \"https://api.githubcopilot.com/mcp/\",  # você usaria o endpoint público de GitHub\n",
    "        \"headers\": {\n",
    "            \"Authorization\": f\"Bearer {os.getenv('GITHUB_ACESS_TOKEN')}\"\n",
    "        }\n",
    "    }   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCP Client\n",
    "client = MultiServerMCPClient(mcp_config)\n",
    "tools = await client.get_tools()  # Async tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ MemorySaver() (não InMemorySaver) para async support\n",
    "Agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=tools,\n",
    "    checkpointer=MemorySaver(),  # Suporta async tools corretamente\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231efad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = input(\"Nomeie a conversa: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43672fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"configurable\": {\"thread_id\": thread},\n",
    "    \"stream_mode\": \"values\"  # Ou \"updates\", [\"messages\"], etc.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9abfbd8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'async for' requires an object with __aiter__ method, got generator",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m Agent.stream(\n\u001b[32m      2\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [HumanMessage(content=\u001b[33m\"\u001b[39m\u001b[33mBusca pra mim o repositório mais recente do Baldros.\u001b[39m\u001b[33m\"\u001b[39m)]},  \u001b[38;5;66;03m# ✅ dict com 'messages'\u001b[39;00m\n\u001b[32m      3\u001b[39m     config=config  \u001b[38;5;66;03m# ✅ stream_mode dentro de config\u001b[39;00m\n\u001b[32m      4\u001b[39m ):\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# ✅ Para \"values\"/\"updates\": chunk é dict[node_name, state_delta]\u001b[39;00m\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# Acesse mensagens: chunk['model']['messages'][-1].content ou similar\u001b[39;00m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m chunk:\n\u001b[32m      8\u001b[39m         last_msg = chunk[\u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m'\u001b[39m][-\u001b[32m1\u001b[39m]\n",
      "\u001b[31mTypeError\u001b[39m: 'async for' requires an object with __aiter__ method, got generator"
     ]
    }
   ],
   "source": [
    "async for chunk in Agent.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"Busca pra mim o repositório mais recente do Baldros.\")]},  # ✅ dict com 'messages'\n",
    "    config=config  # ✅ stream_mode dentro de config\n",
    "):\n",
    "    # ✅ Para \"values\"/\"updates\": chunk é dict[node_name, state_delta]\n",
    "    # Acesse mensagens: chunk['model']['messages'][-1].content ou similar\n",
    "    if 'model' in chunk:\n",
    "        last_msg = chunk['model']['messages'][-1]\n",
    "        print(last_msg.content if hasattr(last_msg, 'content') else str(last_msg))\n",
    "    else:\n",
    "        print(chunk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PSWindows",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
